{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35f0fc39-b189-477c-b9ec-e6c466eafafd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T11:18:37.574434Z",
     "iopub.status.busy": "2021-07-20T11:18:37.574217Z",
     "iopub.status.idle": "2021-07-20T11:18:37.579136Z",
     "shell.execute_reply": "2021-07-20T11:18:37.578701Z",
     "shell.execute_reply.started": "2021-07-20T11:18:37.574413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/yeomsara/YSR/python/05_BATCH_Keyword_Trend_Analy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/yeomsara/YSR/python/05_BATCH_Keyword_Trend_Analy.py\n",
    "## Anal Env.\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import pytz\n",
    "import copy\n",
    "from collections import Counter\n",
    "from emoji       import UNICODE_EMOJI\n",
    "from functools   import reduce\n",
    "import sys\n",
    "sys.path.append('/home/ez-flow/big_data/python/')\n",
    "import bigquery_sql_load as sql_loader\n",
    "import bigquery_etl as bq\n",
    "import operator\n",
    "import time\n",
    "from   datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# NLP Env.\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem     import WordNetLemmatizer,PorterStemmer,LancasterStemmer\n",
    "from nltk.corpus   import wordnet\n",
    "from nltk.corpus   import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import string,re\n",
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import ray\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#glove\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Vis Env.\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GCP Env.\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator\n",
    "\n",
    "# Coding Env.\n",
    "import warnings\n",
    "credentials, project_id = google.auth.default(\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id )\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ray.shutdown()\n",
    "#initialization ray\n",
    "num_logit_cpus = psutil.cpu_count()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=num_logit_cpus)\n",
    "\n",
    "\n",
    "# Load Data FROM Big Query(db connection)\n",
    "def convert_lowercase(df):\n",
    "    df_1 =  df.apply(lambda x: x.astype(str).str.lower() if(x.dtype == 'object') else x)\n",
    "    upper_list = ['reviewId','asin','size','cmpl_fc1_cd']\n",
    "    cols = list(set(upper_list)& set(df_1.columns))\n",
    "    df_1[cols] = df_1[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df_1\n",
    "\n",
    "def convert_uppercase(df):\n",
    "    upper_list = ['reviewId','asin']\n",
    "    cols = list(set(upper_list)& set(df.columns))\n",
    "    df[cols] = df[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df\n",
    "\n",
    "def lemmatize(x) : \n",
    "    if len(x.split(' ')) > 1 : # MWE\n",
    "        tmp_x = x.split(' ')\n",
    "        tmp_x = [WordNetLemmatizer().lemmatize(y, pos='v') for y in tmp_x ]\n",
    "        tokenized_string = \" \".join(tmp_x)\n",
    "    else : # Single\n",
    "        tokenized_string = WordNetLemmatizer().lemmatize(x, pos='v')\n",
    "        \n",
    "    return tokenized_string\n",
    "\n",
    "\n",
    "def cal_trend_analysis_df(keyword_df,sentiment):\n",
    "    rem     = string.punctuation\n",
    "    pattern = r\"[{}]\".format(rem)\n",
    "\n",
    "    df = keyword_df\n",
    "    df['word']   = df['word'].str.replace(pattern, '')\n",
    "    df['factor_yn'] = ''\n",
    "    if sentiment == 0:\n",
    "        factor_col_name = 'cmpl_factor'\n",
    "        df.loc[((df['word'].isin(cmpl_fc_list))),'factor_yn']    = 'Y'\n",
    "        df.loc[(~(df['word'].isin(cmpl_fc_list))),'factor_yn']   = 'N'\n",
    "        df_1         = df.groupby(['yearmonth','year','month','factor_yn']+[factor_col_name])['review_cnt'].sum().reset_index().sort_values(['yearmonth',factor_col_name],ascending=[True,True])\n",
    "        df_1         = df_1.rename(columns={factor_col_name:f'factor'})\n",
    "        df_1['sentiment'] = 'negative'\n",
    "    elif sentiment == 1:\n",
    "        factor_col_name = 'shiny_factor'\n",
    "        df.loc[((df['word'].isin(shiny_factor_list))),'factor_yn']    = 'Y'\n",
    "        df.loc[(~(df['word'].isin(shiny_factor_list))),'factor_yn']   = 'N'\n",
    "        df_1         = df.groupby(['yearmonth','year','month','factor_yn']+[factor_col_name])['review_cnt'].sum().reset_index().sort_values(['yearmonth',factor_col_name],ascending=[True,True])\n",
    "        df_1         = df_1.rename(columns={factor_col_name:f'factor'})\n",
    "        df_1['sentiment'] = 'positive'\n",
    "    df_1['diff'] = df_1.groupby(['factor'])['review_cnt'].diff().fillna(0)\n",
    "    window_size  = 2\n",
    "    cols_name    = 'diff_ma%s'%(window_size)\n",
    "    df_1[cols_name]   = df_1.groupby('factor')['diff'].transform(lambda x: x.rolling(window_size, 1).mean())\n",
    "    \n",
    "    return cols_name,df_1\n",
    "\n",
    "def make_regidate(regi_df):\n",
    "    regidate     = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    regi_df['regidate'] = regidate\n",
    "    regi_df['regidate'] = pd.to_datetime(regi_df['regidate'])\n",
    "    return regi_df\n",
    "\n",
    "\n",
    "try:\n",
    "    print(' (Setp 1-1) Load Data')\n",
    "    ## complain Factor \n",
    "    factor_sql    = sql_loader.load_sql(2)\n",
    "    cpl_factor    = convert_lowercase(bq.select_query(factor_sql))\n",
    "    cpl_factor    = cpl_factor[~cpl_factor['cmpl_fc1'].isin(['none'])]\n",
    "    cpl_factor['lemma'] = cpl_factor['synonym'].apply(lambda x : lemmatize(x))\n",
    "    cpl_factor['porter_stem'] = cpl_factor['synonym'].apply(lambda x : PorterStemmer().stem(x))\n",
    "    cpl_factor =  cpl_factor.drop(columns=['category']).drop_duplicates()\n",
    "    cf2_factor = cpl_factor[['cmpl_fc1','cmpl_fc2']].reset_index(drop=True)\n",
    "    syn_factor = cpl_factor[['cmpl_fc1','synonym']].reset_index(drop=True)\n",
    "    syn_factor = syn_factor.rename(columns={'synonym':'cmpl_fc2'} )\n",
    "    lem_factor = cpl_factor[['cmpl_fc1','lemma']].reset_index(drop=True)\n",
    "    lem_factor = lem_factor.rename(columns={'lemma':'cmpl_fc2'} )\n",
    "    cpl_factor_merge = pd.concat([cf2_factor,syn_factor,lem_factor]).drop_duplicates()\n",
    "    cpl_factor_merge = cpl_factor_merge[~cpl_factor_merge['cmpl_fc2'].isin(['none'])]\n",
    "    cf1_factor = cpl_factor_merge[['cmpl_fc1']].drop_duplicates()\n",
    "    cmpl_fc_list  = list(set(cpl_factor['cmpl_fc1'].unique().tolist())|set(cpl_factor['cmpl_fc2'].unique().tolist())|set(cpl_factor['synonym'].unique().tolist())\\\n",
    "                    |set(cpl_factor['lemma'].unique().tolist()))\n",
    "    shiny_factor =  convert_lowercase(bq.select_query(sql_loader.load_sql(6)))\n",
    "    shiny_factor_list = list(set(shiny_factor['shiny_factor'].unique().tolist())|set(shiny_factor['keyword'].unique().tolist()))\n",
    "\n",
    "\n",
    "    negative_sql,positive_sql,start_ym,YM = sql_loader.Keyword_Trend_SQL()\n",
    "    print(f'========================================================================')\n",
    "    print(f'''Keyword Trend Analysis  :  '{start_ym}'~'{YM}'(target yearmonth) ''')\n",
    "    print(f'========================================================================')\n",
    "\n",
    "    print(f' (Step 2-1) Calculate keyword diff and moving average (default 2M)')    \n",
    "    ## Negative Keyword Pre-processing\n",
    "    negative_df                           = convert_lowercase(bq.select_query(negative_sql)).sort_values(['word','yearmonth'],ascending=[True,True])\n",
    "    ## join (Cmpl_fc2 + synonym + lemma)\n",
    "    neg_keyword_df_join = pd.merge(negative_df,cpl_factor_merge,how='left',left_on='word',right_on=['cmpl_fc2'])\n",
    "    ## join (Cmpl_fc1)\n",
    "    neg_keyword_df_join = pd.merge(neg_keyword_df_join,cf1_factor,how='left',left_on='word',right_on=['cmpl_fc1'])\n",
    "    neg_keyword_df_join.loc[neg_keyword_df_join['cmpl_fc1_x'].isnull(),'cmpl_fc1_x'] = neg_keyword_df_join['cmpl_fc1_y']\n",
    "    neg_keyword_df_join['cmpl_factor'] = neg_keyword_df_join['cmpl_fc1_x']\n",
    "    neg_keyword_df_join = neg_keyword_df_join.drop(columns=['cmpl_fc2','cmpl_fc1_x','cmpl_fc1_y'])\n",
    "    neg_keyword_df_join[~neg_keyword_df_join['cmpl_factor'].isnull()]\n",
    "    negative_df = neg_keyword_df_join\n",
    "    negative_df.loc[negative_df['cmpl_factor'].isnull(),'cmpl_factor']   = negative_df['word']\n",
    "    ## Positive Keyword Pre-processing\n",
    "    positive_df                           = convert_lowercase(bq.select_query(positive_sql)).sort_values(['word','yearmonth'],ascending=[True,True])\n",
    "    positive_df.loc[positive_df['shiny_factor'].isnull(),'shiny_factor'] = positive_df['word']\n",
    "    positive_df.loc[(positive_df['shiny_factor'].isin(['none']))|(positive_df['shiny_factor'].isnull()),'shiny_factor'] = positive_df['word']\n",
    "    cols_name,neg_trend_df                = cal_trend_analysis_df(negative_df,0)\n",
    "    cols_name,pos_trend_df                = cal_trend_analysis_df(positive_df,1)\n",
    "    all_trend_df = pd.concat([neg_trend_df.round(1),pos_trend_df.round(1)])\n",
    "\n",
    "    # # Make registration date\n",
    "    all_trend_df['ym_rvw_cnt_rank'] = all_trend_df.groupby(['yearmonth','factor_yn','sentiment'])['review_cnt'].rank(ascending=False,method='first')\n",
    "    all_trend_df['ym_diff_rank'] = all_trend_df.groupby(['yearmonth','factor_yn','sentiment'])['diff'].rank(ascending=False,method='first')\n",
    "    all_trend_df = make_regidate(all_trend_df)\n",
    "    all_trend_tbl_name = 'taxonomy.monthly_all_keyword_trend_anal'\n",
    "    bq.to_bigquery(all_trend_tbl_name,all_trend_df)\n",
    "    all_trend_df = all_trend_df.drop(columns=['regidate'])\n",
    "    print(f'''         >> Success '{all_trend_tbl_name}' DataBase Upload''')\n",
    "\n",
    "\n",
    "    print(' (Step 3-1) Top30 Keyword Filtering Increased / Decreased by sentiment & factor yn') \n",
    "    factor_df_trend              = all_trend_df[(all_trend_df['factor_yn'] =='Y')]\n",
    "    none_factor_trend            = all_trend_df[(all_trend_df['factor_yn'] =='N')]\n",
    "    ## top30 increase factor alert list \n",
    "    increase_factor_trend                    = factor_df_trend[(factor_df_trend['yearmonth']==YM) & (factor_df_trend['diff'] > 0)].sort_values(['factor_yn','ym_diff_rank'],ascending=[False,True])\n",
    "    increase_non_factor_trend                = none_factor_trend[(none_factor_trend['yearmonth']==YM) & (none_factor_trend['diff'] > 0) & (none_factor_trend['ym_diff_rank'] <= 30)].sort_values(['factor_yn','ym_diff_rank'],ascending=[False,True])\n",
    "    increase_factor_df  = pd.concat([increase_factor_trend,increase_non_factor_trend])\n",
    "    increase_factor_df['trend_cat'] = 'increased'\n",
    "\n",
    "    ## top30 decrease factor alert List\n",
    "    decrease_factor_trend                    = factor_df_trend[(factor_df_trend['yearmonth']==YM) & (factor_df_trend['diff'] < 0)].sort_values(['factor_yn','ym_diff_rank'],ascending=[False,False])\n",
    "    decrease_non_factor_trend                = none_factor_trend[(none_factor_trend['yearmonth']==YM) & (none_factor_trend['diff'] < 0) ].sort_values(['factor_yn','ym_diff_rank'],ascending=[False,False])\n",
    "    decrease_non_factor_trend                = decrease_non_factor_trend.groupby(['sentiment']).apply(lambda x: x.nlargest(30,['ym_diff_rank'])).reset_index(drop=True)\n",
    "    decrease_factor_df  = pd.concat([decrease_factor_trend,decrease_non_factor_trend])\n",
    "    decrease_factor_df['trend_cat'] = 'decreased'\n",
    "\n",
    "    ## concat increased / decreased trend \n",
    "    trend_df_total = pd.concat([increase_factor_df.reset_index(drop=True),decrease_factor_df.reset_index(drop=True)])\n",
    "    ## Make registration date\n",
    "    trend_df_total = make_regidate(trend_df_total)\n",
    "    alert_trend_tbl_name = 'taxonomy.monthly_alert_keyword_trend_anal'\n",
    "    bq.to_bigquery(alert_trend_tbl_name,trend_df_total)\n",
    "    print(f'''         >> Success '{alert_trend_tbl_name}' DataBase Upload''')\n",
    "\n",
    "    print(' (Step 4-1) Top30 reviews count keyword (complain factor / Non-complain Factor)') \n",
    "    ##top30 keyword review count by Factor or non_Factor\n",
    "    factor_top_review_count_word     = factor_df_trend[(factor_df_trend['yearmonth']==YM)].sort_values(['sentiment','ym_rvw_cnt_rank'],ascending=[False,True]).drop(columns=['diff','diff_ma2','ym_diff_rank'])\n",
    "    nonfactor_top_review_count_word  = none_factor_trend[(none_factor_trend['yearmonth']==YM)].sort_values(['sentiment','ym_rvw_cnt_rank'],ascending=[False,True]).drop(columns=['diff','diff_ma2','ym_diff_rank'])\n",
    "    nonfactor_top_review_count_word  = nonfactor_top_review_count_word.groupby(['sentiment']).apply(lambda x: x.nsmallest(30,['ym_rvw_cnt_rank'])).reset_index(drop=True)\n",
    "    top30_word_review_cnt_df         = pd.concat([factor_top_review_count_word.reset_index(drop=True),nonfactor_top_review_count_word.reset_index(drop=True)])\n",
    "    top30_word_review_cnt_df         = top30_word_review_cnt_df.reindex(columns=['yearmonth', 'year', 'month', 'factor_yn', 'factor', 'review_cnt','ym_rvw_cnt_rank', 'sentiment'])\n",
    "    ## Make registration date\n",
    "    top30_word_review_cnt_df = make_regidate(top30_word_review_cnt_df)\n",
    "\n",
    "    top30_review_cnt_tbl_name = 'taxonomy.monthly_top_review_cnt_keyword_anal'\n",
    "    bq.to_bigquery(top30_review_cnt_tbl_name,top30_word_review_cnt_df)\n",
    "    print(f'''         >> Success '{top30_review_cnt_tbl_name}' DataBase Upload''')\n",
    "except Exception as e:\n",
    "    print(f'Keyword Trend Error : {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
