{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea9bdac2-043e-401b-b899-e4180d4f0362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T11:18:23.838452Z",
     "iopub.status.busy": "2021-07-20T11:18:23.838240Z",
     "iopub.status.idle": "2021-07-20T11:18:23.843646Z",
     "shell.execute_reply": "2021-07-20T11:18:23.843195Z",
     "shell.execute_reply.started": "2021-07-20T11:18:23.838431Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/yeomsara/YSR/python/02_BATCH_Bert_CF_Predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/yeomsara/YSR/python/02_BATCH_Bert_CF_Predict.py\n",
    "import sys\n",
    "sys.path.append('/home/ez-flow/big_data/python/')\n",
    "import bigquery_etl as bq\n",
    "import confusion_matrix_customized as cm_customize\n",
    "import matplotlib.pyplot as plt\n",
    "import bigquery_sql_load as sql_loader\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from   datetime    import datetime\n",
    "import pytz\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "# Coding Env.\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def convert_lowercase(df):\n",
    "    df_1 =  df.apply(lambda x: x.astype(str).str.lower() if(x.dtype == 'object') else x)\n",
    "    upper_list = ['reviewId','asin','size','cmpl_fc1_cd']\n",
    "    cols = list(set(upper_list)& set(df_1.columns))\n",
    "    df_1[cols] = df_1[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df_1\n",
    "\n",
    "def convert_uppercase(df):\n",
    "    upper_list = ['reviewId','asin','sku']\n",
    "    cols = list(set(upper_list)& set(df.columns))\n",
    "    df[cols] = df[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df\n",
    "\n",
    "# (Step1-1) data filtering  \n",
    "def make_anal_df(df,senti):\n",
    "    ## sentiment (0) : negative review | (1) : positive review\n",
    "    df['rat_sentiment'] =  np.where(df['rating']<=2, 0,1) ## give rating sentiment 1~2 star = neg /  5 star = pos\n",
    "    ## combind title + review_text\n",
    "    df['review_text']   = df[['title','review_text']].astype(str).sum(axis=1)\n",
    "    if senti == 0:\n",
    "        df_1 = df[(df['rat_sentiment']==0)]\n",
    "    else :\n",
    "        df_1 = df[(df['rat_sentiment']==1)]    \n",
    "    df_1['date'] = pd.to_datetime(df_1.date)\n",
    "    df_1['yearmonth'] = df_1['date'].dt.strftime('%Y%m')\n",
    "    df_1['year']      = df_1['date'].dt.strftime('%Y')\n",
    "    df_1['month']     = df_1['date'].dt.strftime('%m')\n",
    "    df_1 = convert_uppercase(df_1)\n",
    "    return df_1\n",
    "\n",
    "## input data Embeding for bert \n",
    "def make_bert_input_embeding(input_df,x_cols):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "    print(input_df.shape)\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        input_df[x_cols].values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True, \n",
    "        max_length=256, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids_train = encoded_data['input_ids']\n",
    "    attention_masks_train = encoded_data['attention_mask']\n",
    "    \n",
    "    dataset = TensorDataset(input_ids_train, attention_masks_train)\n",
    "    dataset_loader = DataLoader(dataset, \n",
    "                                   sampler=SequentialSampler(dataset), \n",
    "                                   batch_size=batch_size)\n",
    "    return dataset_loader\n",
    "    \n",
    "## predict bert Model\n",
    "def predict_bert(model,dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # logits nomalized (log Probability -> softmax probability)\n",
    "        logits = torch.nn.functional.softmax(outputs[0],dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "def make_regidate(regi_df):\n",
    "    regidate     = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    regi_df['regidate'] = regidate\n",
    "    regi_df['regidate'] = pd.to_datetime(regi_df['regidate'])\n",
    "    return regi_df\n",
    "\n",
    "## Convert Predict Class complain factor type\n",
    "def convert_predict_class(reviewId_df,preds):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    print(label_dict_inverse)\n",
    "    second_flat = np.array(preds).argpartition(-2)[:,-2]\n",
    "    third_flat  = np.array(preds).argpartition(-3)[:,-3]\n",
    "    preds_flat  = np.argmax(np.array(preds), axis=1).flatten()\n",
    "    preds['y_pred'] = preds_flat\n",
    "    preds['second_pred'] = second_flat\n",
    "    preds['third_pred'] = third_flat\n",
    "    preds['y_pred_class'] = preds_flat\n",
    "    preds['second_class'] = second_flat\n",
    "    preds['third_class'] = third_flat\n",
    "    preds = preds.replace({'y_pred_class':label_dict_inverse,\n",
    "                           'second_class':label_dict_inverse,\n",
    "                           'third_class':label_dict_inverse})\n",
    "    preds['reviewId'] = reviewId_df['reviewId'].tolist()\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "##### Pipeline Start ######\n",
    "try:\n",
    "    ## (Setp 1-1) Load Data\n",
    "    bert_train_sql,start_date,end_date,start_ym,end_ym   = sql_loader.review_input_load_sql()\n",
    "\n",
    "    print(f'''==================================================================================''')\n",
    "    print(f''' bert System target initialTime {end_ym} '{end_date}'(batch time)''')\n",
    "    print(f''' bert System date yearmonth     between '{start_ym}' and '{end_ym}' ''')      \n",
    "    print(f'''==================================================================================''')\n",
    "    print(' (Setp 1-1) Load Data')\n",
    "    df                = convert_lowercase(bq.select_query(bert_train_sql))\n",
    "    df_1 = make_anal_df(df,0)\n",
    "    print('           Original review_data        : ' ,df.shape)\n",
    "    print(f'           rating(1~2) negative review : {df_1.shape}')\n",
    "    print(f'           yeamonth new negative review_data : \\n' ,df_1.groupby('yearmonth')['reviewId'].count().reset_index())\n",
    "    test_df     = df_1.drop_duplicates('reviewId')\n",
    "    test_df     = test_df[['reviewId','review_text']]\n",
    "    reviewId_df = test_df[['reviewId']]\n",
    "\n",
    "    ##(Step 1-2) Load complain factor priority and label\n",
    "    ## make Label Dictionary (priority-> class number)\n",
    "    label_dict = {  'recovery'        : 0, \n",
    "                    'durability'      : 1,\n",
    "                    'defect'          : 2, \n",
    "                    'too hard'        : 3,  \n",
    "                    'too soft'        : 4, \n",
    "                    'missing parts'   : 5,\n",
    "                    'odor'            : 6,\n",
    "                    'sound'           : 7,\n",
    "                    'uncomfortable'   : 8,\n",
    "                    'size issue'      : 9,\n",
    "                    'shipping damage' : 10,\n",
    "                    'delivery'        : 11,\n",
    "                    'fiberglass'      : 12,\n",
    "                    'hard to set up'  : 13, \n",
    "                    'slipping'        : 14, \n",
    "                    'cover issue'     : 15, \n",
    "                    'customer service': 16, \n",
    "                    'springs felt'    : 17,\n",
    "                    'overall quality' : 18,\n",
    "                    'no support'      : 19,\n",
    "                    'customer error'  : 20,\n",
    "                    'structure design': 21,\n",
    "                    'others'          : 22, \n",
    "               }\n",
    "\n",
    "\n",
    "    ## (Step 2-1) Embeding Input DF\n",
    "    print(' (Step 2-1) Embeding Input DF')\n",
    "    batch_size  = 5\n",
    "    input_x     = make_bert_input_embeding(test_df,'review_text')\n",
    "    model       = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    ## bert model directory\n",
    "    model_dir = '/home/ez-flow/big_data/model/CF_Bert_Operation.model'\n",
    "    model.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "    ## (Step 3-1) Predict Bert Model \n",
    "    print(' (Step 3-1) Predict Bert Model ')\n",
    "    test_predictions = predict_bert(model,input_x)\n",
    "    predict_df       = pd.DataFrame(test_predictions)\n",
    "    predict_df.columns = list(label_dict)\n",
    "    test_y_pred = convert_predict_class(reviewId_df,predict_df)\n",
    "\n",
    "\n",
    "    ## (Step 4-1) Make Output DataFrame\n",
    "    print(' (Step 4-1) Make Output DataFrame ')\n",
    "    test_y_pred = test_y_pred.reindex(columns=['reviewId','y_pred','second_pred','third_pred','y_pred_class','second_class','third_class',\n",
    "                                               'recovery', 'durability', 'defect', 'too soft', 'too hard',\n",
    "                                               'missing parts', 'odor', 'sound', 'uncomfortable', 'size issue',\n",
    "                                               'shipping damage', 'delivery', 'fiberglass', 'hard to set up',\n",
    "                                               'slipping', 'cover issue', 'customer service', 'springs felt',\n",
    "                                               'overall quality', 'no support','structure design', 'customer error', 'others'])\n",
    "    test_y_pred['cum_prob_top3'] = [test_predictions[i,test_y_pred[['y_pred','second_pred','third_pred']].to_numpy()[i,:]].sum() for i in range(0,test_y_pred.shape[0])]\n",
    "    test_y_pred['y_pred_prob'] = [test_predictions[i,test_y_pred[['y_pred','second_pred','third_pred']].to_numpy()[i,:]].tolist() for i in range(0,test_y_pred.shape[0])]\n",
    "\n",
    "    label_cmpl_cf_dict = {  'recovery'         : 'CF011', \n",
    "                            'durability'       : 'CF001',\n",
    "                            'defect'           : 'CF002', \n",
    "                            'too soft'         : 'CF013', \n",
    "                            'too hard'         : 'CF012',\n",
    "                            'missing parts'    : 'CF003',\n",
    "                            'odor'             : 'CF017',\n",
    "                            'sound'            : 'CF004',\n",
    "                            'uncomfortable'    : 'CF014',\n",
    "                            'size issue'       : 'CF008',\n",
    "                            'shipping damage'  : 'CF018',\n",
    "                            'delivery'         : 'CF019',\n",
    "                            'fiberglass'       : 'CF007',\n",
    "                            'hard to set up'   : 'CF010', \n",
    "                            'slipping'         : 'CF009', \n",
    "                            'cover issue'      : 'CF006', \n",
    "                            'customer service' : 'CF021', \n",
    "                            'springs felt'     : 'CF016',\n",
    "                            'overall quality'  : 'CF005',\n",
    "                            'no support'       : 'CF015',\n",
    "                            'customer error'   : 'CF020',\n",
    "                            'others'           : 'CF999', \n",
    "                            'structure design' : 'CF022'\n",
    "                       }\n",
    "\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    bert_result = test_y_pred.rename(columns=label_cmpl_cf_dict)\n",
    "\n",
    "\n",
    "    bert_result['y_pred_list']  = bert_result[['y_pred_class','second_class','third_class']].to_numpy().tolist()\n",
    "    bert_result = pd.merge(bert_result,df_1[['reviewId','asin','date','yearmonth']].drop_duplicates(),how='left',on=['reviewId'])\n",
    "    bert_result = bert_result.reindex(columns=['reviewId', 'asin', 'date', 'yearmonth', 'y_pred', 'second_pred', 'third_pred', 'y_pred_class',\n",
    "                                                 'second_class', 'third_class', 'CF011', 'CF001', 'CF002', 'CF013',\n",
    "                                                 'CF012', 'CF003', 'CF017', 'CF004', 'CF014', 'CF008', 'CF018', 'CF019',\n",
    "                                                 'CF007', 'CF010', 'CF009', 'CF006', 'CF021', 'CF016', 'CF005', 'CF015',\n",
    "                                                 'CF022', 'CF020', 'CF999', 'cum_prob_top3', 'y_pred_prob','y_pred_list'])\n",
    "    \n",
    "    reviewId_list = \"','\".join(bert_result.reviewId.unique())\n",
    "    ## avoid data duplicates upload delete reviewId and upload\n",
    "    \n",
    "\n",
    "    bert_result = make_regidate(bert_result)\n",
    "    # bert result db upload\n",
    "    print(' (Step 5-1) connection DataBase ')\n",
    "    bert_tbl_name = 'taxonomy.bert_cf1_predict_result'\n",
    "    bq.excute_query(f''' DELETE FROM {bert_tbl_name} WHERE reviewId in({\"'\"+reviewId_list+\"'\"}) ''')\n",
    "    print(f''' (Step 5-2) delete '{bert_tbl_name}' target reviewId count {len(bert_result.reviewId.unique())} ''')\n",
    "    \n",
    "    bq.insert_append_query(bert_tbl_name,bert_result)\n",
    "    print(f'           >> Success Bert Predict Pipeline')\n",
    "    print(f'           >> Success {bert_tbl_name} DataBase Upload')\n",
    "except Exception as e :\n",
    "    print(f'Bert Error : {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
