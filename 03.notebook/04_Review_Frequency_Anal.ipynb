{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ee8f92-1605-4e27-89c3-8a1821878c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T11:19:30.145481Z",
     "iopub.status.busy": "2021-07-20T11:19:30.145251Z",
     "iopub.status.idle": "2021-07-20T11:19:30.151258Z",
     "shell.execute_reply": "2021-07-20T11:19:30.150825Z",
     "shell.execute_reply.started": "2021-07-20T11:19:30.145461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/yeomsara/YSR/python/04_BATCH_Review_Keyword_Analy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/yeomsara/YSR/python/04_BATCH_Review_Keyword_Analy.py\n",
    "## Anal Env.\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import string,re\n",
    "import datetime\n",
    "import pytz\n",
    "import copy\n",
    "from collections import Counter\n",
    "from emoji       import UNICODE_EMOJI\n",
    "from functools   import reduce\n",
    "import sys\n",
    "sys.path.append('/home/ez-flow/big_data/python/')\n",
    "import bigquery_sql_load as sql_loader\n",
    "import bigquery_etl as bq\n",
    "import operator\n",
    "import time\n",
    "from   datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# NLP Env.\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem     import WordNetLemmatizer,PorterStemmer,LancasterStemmer\n",
    "from nltk.corpus   import wordnet\n",
    "from nltk.corpus   import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import ray\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#glove\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Vis Env.\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GCP Env.\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator\n",
    "\n",
    "# Coding Env.\n",
    "import warnings\n",
    "credentials, project_id = google.auth.default(\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id )\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ray.shutdown()\n",
    "#initialization ray\n",
    "num_logit_cpus = psutil.cpu_count()\n",
    "print(f'multiprocessing using {num_logit_cpus} cores')\n",
    "ray.init(ignore_reinit_error=True,num_cpus=num_logit_cpus)\n",
    "\n",
    "\n",
    "# Define List\n",
    "\n",
    "# Load Data \"FROM\" Big Query(db connection)\n",
    "def convert_lowercase(df):\n",
    "    df_1 =  df.apply(lambda x: x.astype(str).str.lower() if(x.dtype == 'object') else x)\n",
    "    return df_1\n",
    "\n",
    "def convert_uppercase(df):\n",
    "    upper_list = ['reviewId','asin','sku','size','cmpl_fc1_cd']\n",
    "    cols = list(set(upper_list)& set(df.columns))\n",
    "    df[cols] = df[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df\n",
    "\n",
    "def top20_df_brand(df):\n",
    "    br_cat_rvw_rank  = pd.pivot_table(df, index = ['brand'], values = ['reviewId'], columns = ['prdct_ctgry_4_5'], aggfunc = ['count'], fill_value = 0, margins = True)#.reset_index()#.to_csv('ddd.csv')\n",
    "    br_rvw_rank_all  = br_cat_rvw_rank['count']['reviewId']['All'].reset_index()\n",
    "    br_rvw_rank_all  = br_rvw_rank_all.loc[(br_rvw_rank_all['brand'] != 'All'),]\n",
    "    br_rvw_rank_all['rank'] = br_rvw_rank_all['All'].rank(ascending=False).astype(int)\n",
    "    br_rvw_rank_all  = br_rvw_rank_all.sort_values(by='rank',ascending=True)\n",
    "    br_rvw_rank_all  = br_rvw_rank_all[0:20]\n",
    "    return br_rvw_rank_all['brand'].tolist()\n",
    "\n",
    "# Lemmatize\n",
    "def lemmatize(x) : \n",
    "    if len(x.split(' ')) > 1 : # MWE\n",
    "        tmp_x = x.split(' ')\n",
    "        tmp_x = [WordNetLemmatizer().lemmatize(y, pos='v') for y in tmp_x ]\n",
    "        tokenized_string = \" \".join(tmp_x)\n",
    "    else : # Single\n",
    "        tokenized_string = WordNetLemmatizer().lemmatize(x, pos='v')\n",
    "        \n",
    "    return tokenized_string\n",
    "\n",
    "# (Step1-1) data filtering  \n",
    "def make_anal_df(df,senti):\n",
    "    ## sentiment (0) : negative review | (1) : positive review\n",
    "    df['rat_sentiment'] =  np.where(df['rating']<=2, 0,1) ## give rating sentiment 1~2 star = neg /  5 star = pos\n",
    "    ## combind title + review_text\n",
    "    df['review_text']   = df[['title','review_text']].astype(str).sum(axis=1)\n",
    "    if senti == 0:\n",
    "        df_1 = df[(df['rat_sentiment']==0)]\n",
    "    else :\n",
    "        df_1 = df[(df['rat_sentiment']==1)]    \n",
    "    df_1['date'] = pd.to_datetime(df_1.date)\n",
    "    df_1['yearmonth'] = df_1['date'].dt.strftime('%Y%m')\n",
    "    df_1['year']      = df_1['date'].dt.strftime('%Y')\n",
    "    df_1['month']     = df_1['date'].dt.strftime('%m')\n",
    "    df_1 = convert_uppercase(df_1)\n",
    "    return df_1\n",
    "\n",
    "\n",
    "#check multiprocessing progress \n",
    "def to_iterator(obj_ids):\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "    \n",
    "def ray_multiprocessing_progress(ray_df):\n",
    "    for x in tqdm(to_iterator(ray_df), total=len(ray_df)):\n",
    "        pass\n",
    "    ray_df  = pd.concat(ray.get(ray_df))\n",
    "    return ray_df\n",
    "\n",
    "def make_regidate(regi_df):\n",
    "    regidate     = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    regi_df['regidate'] = regidate\n",
    "    regi_df['regidate'] = pd.to_datetime(regi_df['regidate'])\n",
    "    return regi_df\n",
    "\n",
    "@ray.remote\n",
    "def reviewId_tokenized_sents(df):\n",
    "    review_df   = df\n",
    "    N_POS_TAG   = ['CC','CD','DT','EX','FW','LS','PDT','POS','PRP','PRP$','TO','WDT','WP','WRB']\n",
    "    Y_POS_TAG   = ['JJ','JJR','JJS','MD','IN','NN','NNS','NNP','NNPS','RB','RBR','RBS','RP','UH','VB','VBG','VBD','VBN','VBP','VBZ']\n",
    "    keyword_df2 = pd.DataFrame()\n",
    "    mwe_tokenizer          = nltk.tokenize.MWETokenizer(mwe,separator=' ')\n",
    "    for i,v in enumerate(review_df['review_text']):\n",
    "        corpus = []\n",
    "        reid = str(review_df.iloc[i].reviewId)\n",
    "        asin = str(review_df.iloc[i].asin)\n",
    "        ym   = str(review_df.iloc[i].yearmonth)\n",
    "        for j in pos_tag(regexp_tokenize(v,\"[\\w']+\")) :\n",
    "            if (j[1] in Y_POS_TAG ) & (len(j[0])>1) & (j[0].isascii()):\n",
    "                    corpus.append(j[0])\n",
    "            tokenized_string       = [WordNetLemmatizer().lemmatize(x,pos='v') for x in corpus]\n",
    "            tokenized_string       = mwe_tokenizer.tokenize(tokenized_string)\n",
    "            tokenized_string       = [i for i in tokenized_string if i not in stop_words]\n",
    "            word_df    = pd.DataFrame(tokenized_string,columns=['word'])\n",
    "            keyword_df = pd.DataFrame.from_dict(Counter(word_df['word']), orient='index').reset_index()\n",
    "            keyword_df['reviewId'] = reid\n",
    "            keyword_df['asin'] = asin\n",
    "            keyword_df['yearmonth'] = ym\n",
    "            if keyword_df.shape[0] > 0:\n",
    "                keyword_df.columns = ['word','count','reviewId','asin','yearmonth']    \n",
    "        keyword_df2    = pd.concat([keyword_df2,keyword_df])\n",
    "        keyword_df2    = keyword_df2[~keyword_df2['word'].isin(stop_words)].sort_values(by='reviewId',ascending=False)\n",
    "        \n",
    "    return keyword_df2\n",
    "\n",
    "##### Pipeline Start ######\n",
    "try:\n",
    "    ##  Data Load\n",
    "    review_sql,start_date,end_date,start_ym,end_ym   = sql_loader.review_input_load_sql()\n",
    "    ##########################################\n",
    "    # sql_cd 1 == stopword sql     \n",
    "    # sql_cd 2 == complain Factor sql     \n",
    "    # sql_cd 3 == bsr_brnad sql     \n",
    "    # sql_cd 4 == part_sql sql    \n",
    "    # sql_cd 5 == taxonomy rule sql \n",
    "    # sql_cd 6 == shiny factor \n",
    "    ##########################################\n",
    "    filter_sql    = sql_loader.load_sql(1)\n",
    "    factor_sql    = sql_loader.load_sql(2)\n",
    "    bsr_brand_sql = sql_loader.load_sql(3)\n",
    "    part_sql      = sql_loader.load_sql(4)\n",
    "    shiny_sql     = sql_loader.load_sql(6)\n",
    "    print(f'''==================================================================================''')\n",
    "    print(f''' keyword Frequency target initialTime between '{start_ym}' and '{end_ym}' â˜…{end_date}(batch time)''')\n",
    "    print(f''' keyword Frequency date yearmonth     between '{start_ym}' and '{end_ym}' ''')      \n",
    "    print(f'''==================================================================================''')\n",
    "    print(' (Setp 1-1) Load data')\n",
    "    ##(P2) Load complain factor dataframe & multi word express\n",
    "    cpl_factor    = convert_lowercase(bq.select_query(factor_sql))\n",
    "    cpl_factor['lemma'] = cpl_factor['synonym'].apply(lambda x : lemmatize(x))\n",
    "    cpl_factor['porter_stem'] = cpl_factor['synonym'].apply(lambda x : PorterStemmer().stem(x))\n",
    "    shiny_factor = convert_lowercase(bq.select_query(shiny_sql))\n",
    "    shiny_factor['lemma'] = shiny_factor['keyword'].apply(lambda x : lemmatize(x))\n",
    "    shiny_factor.columns = ['shiny_factor','word','lemma']\n",
    "\n",
    "    factor_list  = list(set(cpl_factor['cmpl_fc1'].unique().tolist())|set(cpl_factor['cmpl_fc2'].unique().tolist())|set(cpl_factor['synonym'].unique().tolist())\\\n",
    "                         |set(cpl_factor['lemma'].unique().tolist())|set(shiny_factor['word'].unique().tolist())|set(shiny_factor['lemma'].unique().tolist()))\n",
    "    multi_express = list(filter(lambda x: len(x.split(' '))>1 , factor_list))\n",
    "    ## Put this list for MWE Tokenizing\n",
    "    mwe = [tuple( f.split(' ')) for f in multi_express]\n",
    "\n",
    "    ##(P3) Load Review & BSR & Parts & stopwords & taxonomy rule dataframe\n",
    "    bsr_brand_df   = convert_lowercase(bq.select_query(bsr_brand_sql))\n",
    "    top_brand      = list(set(bsr_brand_df['brand'].unique()))\n",
    "    df             = convert_lowercase(bq.select_query(review_sql))\n",
    "    stopword_df    = convert_lowercase(bq.select_query(filter_sql))\n",
    "    parts_df       = convert_lowercase(bq.select_query(part_sql))\n",
    "    parts_df['part_lemma'] = parts_df['part_word'].apply(lambda x : WordNetLemmatizer().lemmatize(x,pos='v'))\n",
    "    parts_list     = list(set(parts_df['part_word'].unique().tolist())|set(parts_df['part_lemma'].unique().tolist()))\n",
    "    parts_list.sort(reverse=False)\n",
    "    stop_words     = list(set(stopword_df['stopword'])|set(parts_list))\n",
    "    df             = convert_lowercase(bq.select_query(review_sql))\n",
    "    df['rat_sentiment'] =  np.where(df['rating']<=2, 0,1) ## give rating sentiment 1~2 star = neg /  5 star = pos\n",
    "    print(f'''           yearmonth between '{start_ym}' and '{end_ym}' review data : ''',df.shape)\n",
    "    print(f'''           yearmonth between '{start_ym}' and '{end_ym}' Positive(rating 5 star) data : ''',df[df['rating'].isin([5])].shape)\n",
    "    print(f'''           yearmonth between '{start_ym}' and '{end_ym}' Negative(rating 1~2 star) data : ''',df[df['rating'].isin([1,2])].shape)\n",
    "    neg_df = make_anal_df(df,0)\n",
    "    pos_df = make_anal_df(df,1)\n",
    "    all_df = pd.concat([neg_df,pos_df])\n",
    "    print(f'           rating(1~2) negative review : {neg_df.shape}')\n",
    "    print(f'           rating(5) positive review   : {pos_df.shape}')\n",
    "    print(f'           all reviews : {all_df.shape}')\n",
    "    print(f'           yeamonth review_data : \\n' ,all_df.groupby(['yearmonth','rat_sentiment'])['reviewId'].count().reset_index())\n",
    "\n",
    "    print(' (Setp 2-1) Tokenized & Make Corpus')\n",
    "    all_keyword_df = [reviewId_tokenized_sents.remote(all_df.loc[(all_df['yearmonth']==j)]) for j in tqdm(all_df['yearmonth'].unique()) ]\n",
    "    all_keyword_df = pd.concat(ray.get(all_keyword_df))\n",
    "#     all_keyword_df = ray_multiprocessing_progress(all_keyword_df)\n",
    "    all_keyword_df = convert_uppercase(all_keyword_df.drop(columns='index').reset_index(drop=True))\n",
    "\n",
    "\n",
    "    key_cols = ['reviewId', 'yearmonth','year', 'month', 'rating']\n",
    "    merge_all_df = convert_uppercase(all_df[key_cols])\n",
    "    all_keyword = pd.merge(all_keyword_df,merge_all_df,how='left',on=['reviewId','yearmonth'])\n",
    "\n",
    "    rem = string.punctuation\n",
    "    pattern = r\"[{}]\".format(rem)\n",
    "    all_keyword['word'] = all_keyword['word'].str.replace(pattern, '')\n",
    "    \n",
    "    print(' (Setp 2-2)  Divided positive & negative')\n",
    "    pos_all_keyword = all_keyword[all_keyword['rating'].isin([5])]\n",
    "    neg_all_keyword = all_keyword[all_keyword['rating'].isin([1,2])]\n",
    "    print(f'           rating(1~2) negative keyword : {neg_all_keyword.shape}')\n",
    "    print(f'           rating(5) positive keyword   : {pos_all_keyword.shape}')\n",
    "    print(f'           neg+pos review keyword       : {all_keyword.shape}')\n",
    "\n",
    "    pos_all_keyword['sentiment'] = 'positive'\n",
    "    neg_all_keyword['sentiment'] = 'negative'\n",
    "\n",
    "    print(' (Setp 3-1)  Join Factor')\n",
    "    cmpl_fc_list2  = pd.DataFrame(list(set(cpl_factor['cmpl_fc1'].unique().tolist())|set(cpl_factor['cmpl_fc2'].unique().tolist())|set(cpl_factor['synonym'].unique().tolist())|set(cpl_factor['lemma'].unique().tolist())),columns=['cmpl_factor'])\n",
    "#     shiny_factor2  = pd.DataFrame(list(set(shiny_factor['shiny_factor'].unique().tolist())|set(shiny_factor['word'].unique().tolist())|set(shiny_factor['lemma'].unique().tolist())),columns=['shiny_factor'])\n",
    "    neg_keyword = pd.merge(neg_all_keyword,cmpl_fc_list2,how='left',left_on=['word'],right_on=['cmpl_factor']).fillna('')\n",
    "    pos_keyword =  pd.merge(pos_all_keyword,shiny_factor[['word','shiny_factor']],how='left',on=['word']).fillna('')\n",
    "\n",
    "    neg_group_cols = ['asin','word','yearmonth','year','month','rating','sentiment','cmpl_factor']\n",
    "    pos_group_cols = ['asin','word','yearmonth','year','month','rating','sentiment','shiny_factor']\n",
    "\n",
    "    print(' (Setp 4-1)  Calculate count of reviews ')\n",
    "    neg_keyword_freq       = neg_keyword.groupby(neg_group_cols)['count'].sum().reset_index()\n",
    "    neg_keyword_review_cnt = neg_keyword.groupby(neg_group_cols)['reviewId'].count().reset_index()\n",
    "    pos_keyword_freq       = pos_keyword.groupby(pos_group_cols)['count'].sum().reset_index()\n",
    "    pos_keyword_review_cnt = pos_keyword.groupby(pos_group_cols)['reviewId'].count().reset_index()\n",
    "\n",
    "    neg_keyword_table = pd.merge(neg_keyword_freq,neg_keyword_review_cnt,how='left',on=neg_group_cols)\n",
    "    pos_keyword_table = pd.merge(pos_keyword_freq,pos_keyword_review_cnt,how='left',on=pos_group_cols)\n",
    "\n",
    "    neg_keyword_table = neg_keyword_table.rename(columns ={'count':'word_cnt','reviewId':'review_cnt'})\n",
    "    pos_keyword_table = pos_keyword_table.rename(columns ={'count':'word_cnt','reviewId':'review_cnt'})\n",
    "    \n",
    "    \n",
    "    print(' (Step 5-1) connection DataBase ')\n",
    "    neg_db_cols = ['asin','yearmonth','year','month','rating','word','cmpl_factor','word_cnt','review_cnt','sentiment']\n",
    "    pos_db_cols = ['asin','yearmonth','year','month','rating','word','shiny_factor','word_cnt','review_cnt','sentiment']\n",
    "    neg_keyword_table = neg_keyword_table.reindex(columns=neg_db_cols)\n",
    "    pos_keyword_table = pos_keyword_table.reindex(columns=pos_db_cols)\n",
    "    \n",
    "    neg_keyword_table =make_regidate(neg_keyword_table) \n",
    "    pos_keyword_table =make_regidate(pos_keyword_table)\n",
    "    \n",
    "    neg_keyword_tbl_name = 'taxonomy.negative_keyword_anal'\n",
    "    pos_keyword_tbl_name = 'taxonomy.positive_keyword_anal'\n",
    "\n",
    "    bq.excute_query(f''' DELETE FROM {neg_keyword_tbl_name} WHERE yearmonth between '{start_ym}' and '{end_ym}' ''')\n",
    "    print(f''' (Step 5-2) delete '{neg_keyword_tbl_name}' table target yearmonth between '{start_ym}' and '{end_ym}' ''')\n",
    "    bq.excute_query(f''' DELETE FROM {pos_keyword_tbl_name} WHERE yearmonth between '{start_ym}' and '{end_ym}' ''')\n",
    "    print(f''' (Step 5-2) delete '{pos_keyword_tbl_name}' table target yearmonth between '{start_ym}' and '{end_ym}' ''')\n",
    "\n",
    "    bq.insert_append_query(neg_keyword_tbl_name,neg_keyword_table)\n",
    "    print(f'Success {neg_keyword_tbl_name} DataBase Upload')\n",
    "    bq.insert_append_query(pos_keyword_tbl_name,pos_keyword_table)\n",
    "    print(f'Success {pos_keyword_tbl_name} DataBase Upload')\n",
    "    print(f'Success Keyword anal Predict Pipeline')\n",
    "except Exception as e :\n",
    "    print(f'Keyword Analysis Error : {e}')\n",
    "finally:\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
