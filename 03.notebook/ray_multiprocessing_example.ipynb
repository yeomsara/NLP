{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a42190-8850-4ac1-ac66-b1ce56c46905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T07:50:11.968967Z",
     "iopub.status.busy": "2021-06-02T07:50:11.968823Z",
     "iopub.status.idle": "2021-06-02T07:50:11.976818Z",
     "shell.execute_reply": "2021-06-02T07:50:11.976470Z",
     "shell.execute_reply.started": "2021-06-02T07:50:11.968951Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import re\n",
    "import psutil\n",
    "import datetime\n",
    "import pytz\n",
    "import copy\n",
    "from collections import Counter\n",
    "from emoji   import UNICODE_EMOJI\n",
    "from functools   import reduce\n",
    "import sys\n",
    "sys.path.append('/home/handeully/')\n",
    "import bigquery_etl as bq\n",
    "import operator\n",
    "import time\n",
    "from IPython.display import display\n",
    "# NLP Env.\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer,LancasterStemmer\n",
    "from nltk.corpus   import wordnet\n",
    "from nltk.corpus   import sentiwordnet as swn\n",
    "from nltk import sent_tokenize,word_tokenize,pos_tag\n",
    "from tqdm.notebook import tqdm\n",
    "# GCP Env.\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Vis Env.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Coding Env.\n",
    "import warnings\n",
    "credentials, project_id = google.auth.default(\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id )\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "#initialization ray\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf200f-917d-4f7b-b8ac-65877f984ab6",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4dd06e-0dc8-4615-ac07-edb5f825a9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T07:50:13.530756Z",
     "iopub.status.busy": "2021-06-02T07:50:13.530536Z",
     "iopub.status.idle": "2021-06-02T07:50:13.568523Z",
     "shell.execute_reply": "2021-06-02T07:50:13.568056Z",
     "shell.execute_reply.started": "2021-06-02T07:50:13.530733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data FROM Big Query(db connection)\n",
    "def convert_lowercase(df):\n",
    "    df_1 =  df.apply(lambda x: x.astype(str).str.lower() if(x.dtype == 'object') else x)\n",
    "    return df_1\n",
    "\n",
    "df1 = pd.read_csv('temp/ray_reviews.csv')\n",
    "cpl_factor  = pd.read_csv('temp/ray_cpl_factor.csv')\n",
    "stopword_df = pd.read_csv('temp/ray_stopword.csv')\n",
    "stop_words  = stopword_df['stopword'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e252-904c-4cf8-9d8a-924c717df057",
   "metadata": {},
   "source": [
    "# Complain Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b23e180-3af7-4a0b-8c83-7e7f29b5d190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T07:50:13.569443Z",
     "iopub.status.busy": "2021-06-02T07:50:13.569294Z",
     "iopub.status.idle": "2021-06-02T07:50:13.574943Z",
     "shell.execute_reply": "2021-06-02T07:50:13.574565Z",
     "shell.execute_reply.started": "2021-06-02T07:50:13.569426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complain factor    :  (4597, 6)\n",
      "all complain factor list  : 1058\n",
      "multi_express lenth : 407\n"
     ]
    }
   ],
   "source": [
    "cmpl_fc_list  = list(set(cpl_factor['cmpl_fc1'].unique().tolist())|set(cpl_factor['cmpl_fc2'].unique().tolist())|set(cpl_factor['synonym'].unique().tolist())|set(cpl_factor['lemma'].unique().tolist()))\n",
    "multi_express = list(filter(lambda x: len(x.split(' '))>1 , cmpl_fc_list))\n",
    "# Put this list for MWE Tokenizing\n",
    "mwe = [tuple(f.split(' ')) for f in multi_express]\n",
    "print('complain factor    : ' ,cpl_factor.shape)\n",
    "print(f'all complain factor list  : {len(cmpl_fc_list)}')\n",
    "print(f'multi_express lenth : {len(mwe)}')\n",
    "# mwe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eff7b-3fde-4f10-9b50-351f2ded6b28",
   "metadata": {},
   "source": [
    "# Tokenized ( Make corpus & word count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b925371a-e472-4d4b-a4bc-e708ecd9bde7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T07:50:45.291663Z",
     "iopub.status.busy": "2021-06-02T07:50:45.291452Z",
     "iopub.status.idle": "2021-06-02T07:50:47.444839Z",
     "shell.execute_reply": "2021-06-02T07:50:47.444408Z",
     "shell.execute_reply.started": "2021-06-02T07:50:45.291642Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-02 07:50:45,299\tINFO worker.py:664 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Count Word DataFrame ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5374d7c070048a38f4d155cf3c1caf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "N_POS_TAG   = ['CC','CD','DT','EX','FW','LS','PDT','POS','PRP','PRP$','TO','WDT','WP','WRB']\n",
    "Y_POS_TAG   = ['JJ','JJR','JJS','MD','IN','NN','NNS','NNP','NNPS','RB','RBR','RBS','RP','UH','VB','VBG','VBD','VBN','VBP','VBZ']\n",
    "    \n",
    "def tokenized_corpus(df):\n",
    "    corpus = []\n",
    "    reviewid = []\n",
    "    # if you wonder that nltk pos tag\n",
    "    # nltk.help.upenn_tagset()\n",
    "    for i,v in enumerate(tqdm(df['review_text'])):\n",
    "        try:\n",
    "#             print(f' index : {i}| text : {v}\\\\n')\n",
    "            word = []\n",
    "            reviewid.append(str(df.iloc[i].reviewId)) \n",
    "           \n",
    "            for j in pos_tag(regexp_tokenize(v,\"[\\w']+\")) :\n",
    "                if (j[1] in Y_POS_TAG ) & (len(j[0])>1)  & (j[0].isascii()):\n",
    "                    word.append(j[0])\n",
    "            tokenized_string       = list(set(word)-set(stop_words))\n",
    "            corpus.append(tokenized_string)\n",
    "        except : \n",
    "            pass\n",
    "    corpus_df = pd.DataFrame({'corpus_list' : corpus,'reviewId' : reviewid})\n",
    "    return corpus_df\n",
    "    \n",
    "# ray step 0) defind function for ray @ray.remote if you declare ray.remote It is possible only ray (multiprocessing)\n",
    "@ray.remote\n",
    "def tokenized_ray_corpus(df):\n",
    "    corpus = []\n",
    "    reviewid = []\n",
    "    # if you wonder that nltk pos tag\n",
    "    # nltk.help.upenn_tagset()\n",
    "    N_POS_TAG   = ['CC','CD','DT','EX','FW','LS','PDT','POS','PRP','PRP$','TO','WDT','WP','WRB']\n",
    "    Y_POS_TAG   = ['JJ','JJR','JJS','MD','IN','NN','NNS','NNP','NNPS','RB','RBR','RBS','RP','UH','VB','VBG','VBD','VBN','VBP','VBZ']\n",
    "    for i,v in enumerate(df['review_text']):\n",
    "        try:\n",
    "#         print(f' index : {i}| text : {v}\\\\n')\n",
    "            word = []\n",
    "            reviewid.append(str(df.iloc[i].reviewId)) \n",
    "            for j in pos_tag(regexp_tokenize(v,\"[\\w']+\")) :\n",
    "                if (j[1] in Y_POS_TAG ) & (len(j[0])>1)  & (j[0].isascii()):\n",
    "                    word.append(j[0])\n",
    "            mwe_tokenizer          = nltk.tokenize.MWETokenizer(mwe,separator=' ')\n",
    "            tokenized_string       = mwe_tokenizer.tokenize(word)\n",
    "            tokenized_string       = list(set(tokenized_string)-set(stop_words))\n",
    "            corpus.append(tokenized_string)\n",
    "        except : \n",
    "            pass\n",
    "        corpus_df = pd.DataFrame({'corpus_list' : corpus,'reviewId' : reviewid})\n",
    "    return corpus_df\n",
    "    \n",
    "\n",
    "#check multiprocessing progress \n",
    "def to_iterator(obj_ids):\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "\n",
    "def ray_multiprocessing_progress(ray_df):\n",
    "    for x in tqdm(to_iterator(ray_df), total=len(ray_df)):\n",
    "        pass\n",
    "    ray_df  = pd.concat(ray.get(ray_df))\n",
    "    return ray_df\n",
    "\n",
    "\n",
    "#==> (single core example) for compare with single and ray \n",
    "# make corpus df (ex. ['word','word2','word3'])\n",
    "print('================ Make Corpus ===============')\n",
    "corpus_df    = tokenized_corpus(df)\n",
    "print('============================================')\n",
    "\n",
    "## ==> â–   make corpus using ray multiprocessing \n",
    "## ray step 1) setting for number of cpu cores if num_cpus is psutill.cpu_count() ray uses 'full core'  \n",
    "num_logit_cpus = psutil.cpu_count()\n",
    "## ray step 2) When starting ray, you have to ray init (you have to ray.init(num_cpus=n) in advance \n",
    "ray.init(ignore_reinit_error=True,num_cpus=num_logit_cpus)\n",
    "print('================ Count Word DataFrame ===============')\n",
    "## ray step 3) ray start example ( loop is for chunk size )\n",
    "count_df = [tokenized_ray_corpus.remote(df1[df1['brand']==i]) for i in df1['brand'].unique()]\n",
    "count_df = ray_multiprocessing_progress(count_df)\n",
    "print('=====================================================')\n",
    "\n",
    "#ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
