{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bb260d-56c6-4756-8f77-e3823a2f6a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T11:18:18.422254Z",
     "iopub.status.busy": "2021-07-20T11:18:18.422030Z",
     "iopub.status.idle": "2021-07-20T11:18:18.430312Z",
     "shell.execute_reply": "2021-07-20T11:18:18.429827Z",
     "shell.execute_reply.started": "2021-07-20T11:18:18.422231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python/01_BATCH_Taxonomy_CF_Similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile python/01_BATCH_Taxonomy_CF_Similarity.py\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import pytz\n",
    "import copy\n",
    "from collections import Counter\n",
    "from emoji   import UNICODE_EMOJI\n",
    "from functools   import reduce\n",
    "import sys\n",
    "sys.path.append('/home/ez-flow/big_data/python/')\n",
    "import confusion_matrix_customized as cm_customize\n",
    "import bigquery_sql_load as sql_loader\n",
    "import bigquery_etl as bq\n",
    "import operator\n",
    "import time\n",
    "from IPython.display import display\n",
    "# NLP Env.\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer,LancasterStemmer\n",
    "from nltk.corpus   import wordnet\n",
    "from nltk.corpus   import sentiwordnet as swn\n",
    "from nltk import sent_tokenize,word_tokenize,pos_tag\n",
    "from sklearn.metrics import classification_report,plot_confusion_matrix,confusion_matrix,accuracy_score\n",
    "# stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import ray\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "#glove\n",
    "from glove import Corpus,Glove\n",
    "\n",
    "# Vis Env.\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "# GCP Env.\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator\n",
    "\n",
    "# Coding Env.\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "credentials, project_id = google.auth.default(\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id )\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "\n",
    "# Load Data FROM Big Query(db connection)\n",
    "def convert_lowercase(df):\n",
    "    df_1 =  df.apply(lambda x: x.astype(str).str.lower() if(x.dtype == 'object') else x)\n",
    "    upper_list = ['reviewId','asin','size','cmpl_fc1_cd']\n",
    "    cols = list(set(upper_list)& set(df_1.columns))\n",
    "    df_1[cols] = df_1[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df_1\n",
    "\n",
    "def convert_uppercase(df):\n",
    "    upper_list = ['reviewId','asin']\n",
    "    cols = list(set(upper_list)& set(df.columns))\n",
    "    df[cols] = df[cols].apply(lambda x: x.astype(str).str.upper() if(x.dtype == 'object') else x)\n",
    "    return df\n",
    "\n",
    "def top20_df_brand(df):\n",
    "    br_cat_rvw_rank  = pd.pivot_table(df, index = ['brand'], values = ['reviewId'], columns = ['prdct_ctgry_4_5'], aggfunc = ['count'], fill_value = 0, margins = True)#.reset_index()#.to_csv('ddd.csv')\n",
    "    br_rvw_rank_all  = br_cat_rvw_rank['count']['reviewId']['All'].reset_index()\n",
    "    br_rvw_rank_all  = br_rvw_rank_all.loc[(br_rvw_rank_all['brand'] != 'All'),]\n",
    "    br_rvw_rank_all['rank'] = br_rvw_rank_all['All'].rank(ascending=False).astype(int)\n",
    "    br_rvw_rank_all  = br_rvw_rank_all.sort_values(by='rank',ascending=True)\n",
    "    br_rvw_rank_all  = br_rvw_rank_all[0:20]\n",
    "    return br_rvw_rank_all['brand'].tolist()\n",
    "\n",
    "#check multiprocessing progress \n",
    "def to_iterator(obj_ids):\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "        \n",
    "def to_feather_df(df,name):\n",
    "    df.reset_index().to_feather(f'temp/{name}.ftr')\n",
    "    print(f'temp/{name}.ftr에 저장완료')\n",
    "    \n",
    "def ray_multiprocessing_progress(ray_df):\n",
    "    for x in tqdm(to_iterator(ray_df), total=len(ray_df)):\n",
    "        pass\n",
    "    ray_df  = pd.concat(ray.get(ray_df))\n",
    "    return ray_df\n",
    "\n",
    "def make_regidate(regi_df):\n",
    "    regidate     = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    regi_df['regidate'] = regidate\n",
    "    regi_df['regidate'] = pd.to_datetime(regi_df['regidate'])\n",
    "    return regi_df\n",
    "\n",
    "# Lemmatize\n",
    "def lemmatize(x) : \n",
    "    if len(x.split(' ')) > 1 : # MWE\n",
    "        tmp_x = x.split(' ')\n",
    "        tmp_x = [WordNetLemmatizer().lemmatize(y, pos='v') for y in tmp_x ]\n",
    "        tokenized_string = \" \".join(tmp_x)\n",
    "    else : # Single\n",
    "        tokenized_string = WordNetLemmatizer().lemmatize(x, pos='v')\n",
    "        \n",
    "    return tokenized_string\n",
    "\n",
    "\n",
    "# (Step1-1) data filtering  \n",
    "def make_anal_df(df,senti):\n",
    "    ## sentiment (0) : negative review | (1) : positive review\n",
    "    df['rat_sentiment'] =  np.where(df['rating']<=2, 0,1) ## give rating sentiment 1~2 star = neg /  5 star = pos\n",
    "    ## combind title + review_text\n",
    "    df['review_text']   = df[['title','review_text']].astype(str).sum(axis=1)\n",
    "    if senti == 0:\n",
    "        df_1 = df[(df['rat_sentiment']==0)]\n",
    "    else :\n",
    "        df_1 = df[(df['rat_sentiment']==1)]    \n",
    "    df_1['date'] = pd.to_datetime(df_1.date)\n",
    "    df_1['yearmonth'] = df_1['date'].dt.strftime('%Y%m')\n",
    "    df_1['year']      = df_1['date'].dt.strftime('%Y')\n",
    "    df_1['month']     = df_1['date'].dt.strftime('%m')\n",
    "    df_1 = convert_uppercase(df_1)\n",
    "    return df_1\n",
    "\n",
    "\n",
    "# (Step 2-1)grep complain factor3\n",
    "def multiple_regex_list(text):\n",
    "    regexList = [r\"less then [0-9]+ day\",r\"less then [0-9] week\",r\"less then [0-9]+ month\",r\"less then [0-9] year\",\n",
    "                 r\"for [0-9] day\",r\"for [0-9]+ week\",r\"for [0-9]+ month\",r\"for [0-9] year\",'for a few week','for a few day','for a month day','for a year','for a month','for a day','for a week',\n",
    "                 r\"after [0-9]+ day\",r\"after [0-9]+ week\",r\"after [0-9]+ month\",r\"after [0-9] year\",'a few months after','a few days after','a few years after','a few weeks after',\n",
    "                 r\"over [0-9]+ day\",r\"over [0-9]+ week\",r\"over [0-9]+ month\",r\"over [0-9] year\",'within a year','within a week','within a day','within a year','within [0-9]+ month','within [0-9]+ day|within [0-9] week',\n",
    "                 r'after a day',r'after a week',r'after a month',r'after a year','after one year','after one month','after [0-9]+ night',\n",
    "                 r'have had it for about [0-9]+ day',r'have had it for about [0-9]+ month',r'have had it for about [0-9]+ week',r'have had it for about [0-9] year',\n",
    "                 r'[0-9]+ days ago|[0-9]+ day ago',r'[0-9]+ week ago|[0-9]+ weeks ago',r'[0-9]+ month ago|[0-9]+ months ago',r'[0-9] year ago|[0-9] years ago']\n",
    "    gotMatch = []\n",
    "    for regex in regexList:\n",
    "        s = list(set(re.findall(regex, text)))\n",
    "        if len(s) > 0:\n",
    "            gotMatch.append(' '.join(s))\n",
    "    gotMatch = ','.join(gotMatch)\n",
    "    return gotMatch\n",
    "\n",
    "# (Step 2-2)Tokenized & make corpus & grep complain factor3\n",
    "# @ray.remote\n",
    "def tokenized_corpus(df):\n",
    "    corpus    = []\n",
    "    reviewid  = []\n",
    "    category  = []\n",
    "    cmpl_fc3_l  = []\n",
    "    inch_list = []\n",
    "    # if you wonder that nltk pos tag\n",
    "    # nltk.help.upenn_tagset()\n",
    "    N_POS_TAG   = ['CC','DT','EX','FW','LS','PDT','POS','PRP','PRP$','TO','WDT','WP','WRB']\n",
    "    Y_POS_TAG   = ['JJ','JJR','JJS','MD','IN','NN','NNS','NNP','NNPS','RB','RBR','RBS','RP','UH','VB','VBG','VBD','VBN','VBP','VBZ']\n",
    "    for i,v in enumerate(df['review_text']):\n",
    "        try:\n",
    "            inches   = re.findall(r'[0-9]+[\"]|[0-9]+inch|[0-9]+ inch',v)\n",
    "            inches   = list(set(inches))\n",
    "            cmpl_fc3 = multiple_regex_list(v)\n",
    "            cmpl_fc3_l.append(cmpl_fc3)\n",
    "            inch_list.append(inches)\n",
    "            reviewid.append(str(df.iloc[i].reviewId)) \n",
    "            category.append(str(df.iloc[i].category)) \n",
    "            word = []\n",
    "            for j in pos_tag(regexp_tokenize(v,\"[\\w']+\")) :\n",
    "#                 if (len(j[0])>2)  & (j[0].isascii()) & (j[1] in Y_POS_TAG) :\n",
    "                if (j[0].isascii()) & (j[1] in Y_POS_TAG) :\n",
    "                    word.append(j[0])\n",
    "            mwe_tokenizer          = nltk.tokenize.MWETokenizer(mwe,separator=' ')\n",
    "            tokenized_string       = [WordNetLemmatizer().lemmatize(x,pos='v') for x in word ]\n",
    "            tokenized_string       = mwe_tokenizer.tokenize(word)\n",
    "            tokenized_string       = [i for i in tokenized_string if i not in stopwords.words('english')]\n",
    "#             tokenized_string       = [WordNetLemmatizer().lemmatize(x,pos='v') for x in tokenized_string ]\n",
    "            corpus.append(tokenized_string)\n",
    "        except : \n",
    "            pass\n",
    "        corpus_df = pd.DataFrame({ 'corpus_list' : corpus,'reviewId' : reviewid,'category':category ,'cmpl_fc3':cmpl_fc3_l,'inch_regex':inch_list})\n",
    "    return corpus_df\n",
    "\n",
    "#initialization ray\n",
    "num_logit_cpus = psutil.cpu_count()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=num_logit_cpus)\n",
    "\n",
    "#(Step 3-1)Calculating Taxonomy similarity\n",
    "@ray.remote\n",
    "def taxonomy_similarity(cor_df):\n",
    "    sim_result  = pd.DataFrame()\n",
    "    for i in range(0,len(cor_df)):\n",
    "        cat      =  cor_df.category.tolist()[i]\n",
    "        corpus   =  cor_df.corpus_list.tolist()[i]\n",
    "        reviewid =  cor_df.reviewId.tolist()[i]\n",
    "        cat_cpl_fac   = cpl_factor[cpl_factor['category'].isin([cat,'all'])]\n",
    "        cat_cpl_count = cat_cpl_fac.drop_duplicates(['cmpl_fc1','cmpl_fc2'],keep='first').groupby(['cmpl_fc1'])['cmpl_fc2'].count().reset_index()\n",
    "        cat_cpl_count = cat_cpl_fac.groupby(['cmpl_fc1_cd','cmpl_fc1'])['cmpl_fc2'].count().reset_index()\n",
    "        cat_cpl_count['cat_cf_ratio'] = cat_cpl_count['cmpl_fc2']/cat_cpl_count['cmpl_fc2'].sum()\n",
    "        cat_cpl_count.columns         = ['cmpl_fc1_cd','cmpl_fc1','cat_cmpl_fc2_len','cat_cf1_ratio'] \n",
    "        cpl1_list   = cat_cpl_fac.cmpl_fc1.unique()\n",
    "        cpl2_list   = cat_cpl_fac.cmpl_fc2.unique()\n",
    "        for cf1 in cpl1_list : \n",
    "            cf1_intersec = set(corpus)&set([cf1])\n",
    "            #if complain factor1 is in corpus\n",
    "            if len(cf1_intersec) > 0:\n",
    "                cf1_intersec = 0.5\n",
    "                intersec     = list(set(corpus)&set([cf1]))\n",
    "                cf2_df = pd.DataFrame({\n",
    "                                          'category': cat,\n",
    "                                          'reviewId': reviewid,\n",
    "                                          'corpus'  : [corpus],\n",
    "                                          'corpus_len': len(corpus),\n",
    "                                          'cmpl_fc1': cf1,\n",
    "                                          'cf1_intersect': cf1_intersec,\n",
    "                                          'cmpl_fc2_len' : len(cat_cpl_fac[cat_cpl_fac['cmpl_fc1']==cf1].cmpl_fc2.unique()), \n",
    "                                          'cmpl_fc2': cf1,\n",
    "                                          'synonym' : [None],\n",
    "                                          'synonym_len': [0],\n",
    "                                          'syn_intersect':'',\n",
    "                                          'syn_match_len':[0]\n",
    "                                     })\n",
    "                cf2_df     = pd.merge(cf2_df,cat_cpl_count,how='left',on=['cmpl_fc1'])\n",
    "                sim_result = pd.concat([cf2_df,sim_result])\n",
    "            else :\n",
    "                cf1_intersec = 0\n",
    "            for cf2 in cat_cpl_fac[cat_cpl_fac['cmpl_fc1']==cf1].cmpl_fc2.unique() :\n",
    "                syn_list    = [i for i in cat_cpl_fac[cat_cpl_fac['cmpl_fc2'] == cf2].synonym.unique() if i not in ['none',None]]\n",
    "                lemma_list  = [i for i in cat_cpl_fac[cat_cpl_fac['cmpl_fc2'] == cf2].lemma.unique()  if i not in ['none',None]]\n",
    "                syn_list = syn_list+lemma_list\n",
    "                syn_list.append(cf2)\n",
    "                intersec  = set(corpus)&set(syn_list)\n",
    "                if ( len(intersec) > 0 ) :\n",
    "                    cf2_df = pd.DataFrame({\n",
    "                                          'category': cat,\n",
    "                                          'reviewId': reviewid,\n",
    "                                          'corpus'  : [corpus],\n",
    "                                          'corpus_len':len(corpus),\n",
    "                                          'cmpl_fc1': cf1,\n",
    "                                          'cf1_intersect':cf1_intersec,\n",
    "                                          'cmpl_fc2_len' :len(cat_cpl_fac[cat_cpl_fac['cmpl_fc1']==cf1].cmpl_fc2.unique()),                  \n",
    "                                          'cmpl_fc2': cf2,\n",
    "                                          'synonym' : [syn_list],\n",
    "                                          'synonym_len'  : len(syn_list),\n",
    "                                          'syn_intersect':','.join(list(intersec)),\n",
    "                                          'syn_match_len':len(intersec)\n",
    "                                          })\n",
    "                    cf2_df     = pd.merge(cf2_df,cat_cpl_count,how='left',on=['cmpl_fc1'])\n",
    "                    sim_result = pd.concat([cf2_df,sim_result])\n",
    "    return sim_result\n",
    "\n",
    "#(Step 4-1) Add taxonomy rules\n",
    "def taxonomy_rule_chk(simi_rule_df, r_df):\n",
    "    for i in r_df.index : \n",
    "        r_cd = r_df.loc[i, 'rule_code']\n",
    "        cf1 = r_df.loc[i, 'cmpl_fc1']\n",
    "        cf2 = r_df.loc[i, 'cmpl_fc2']\n",
    "        cf3 = r_df.loc[i, 'cmpl_fc3']\n",
    "        inch = r_df.loc[i, 'inch_len']\n",
    "        rule1 = r_df.loc[i, 'rule1']\n",
    "        rule2 = r_df.loc[i, 'rule2']\n",
    "        rule3 = r_df.loc[i, 'rule3']\n",
    "        \n",
    "        if r_cd == 1 :\n",
    "            # duration\n",
    "            conditions = (simi_rule_df['cmpl_fc1'].isin([cf1])) & (~simi_rule_df['cmpl_fc3'].isin(['none','',' ']))\n",
    "            simi_rule_df.loc[conditions, 'rule_check'] = 1\n",
    "            simi_rule_df.loc[conditions, 'rule_score'] = 0.5\n",
    "        elif r_cd == 2 :\n",
    "            # inch\n",
    "            conditions = (simi_rule_df['cmpl_fc1'].isin([cf1])) & (simi_rule_df['cmpl_fc2'].isin([cf2])) & (simi_rule_df['inch_rule']>=2)\n",
    "            simi_rule_df.loc[conditions, 'rule_check'] = 2\n",
    "            simi_rule_df.loc[conditions, 'rule_score'] = 0.5\n",
    "        elif r_cd == 3 :\n",
    "            # meta\n",
    "            conditions = (simi_rule_df['cmpl_fc1'].isin([cf1])) & (simi_rule_df['cmpl_fc2'].isin([cf2])) & (simi_rule_df['corpus'].apply(lambda x : rule3 in x))\n",
    "            simi_rule_df.loc[conditions, 'rule_check'] = 3\n",
    "            simi_rule_df.loc[conditions, 'rule_score'] = 0.5\n",
    "        elif r_cd == 4 :\n",
    "            # filter\n",
    "            conditions = (simi_rule_df['cmpl_fc1'].isin([cf1])) & (simi_rule_df['cmpl_fc2'].isin([cf2])) & (simi_rule_df['cmpl_fc2_list'].apply(lambda x : rule3 not in x))\n",
    "            conditions = conditions | ( simi_rule_df['reviewId'].isin(simi_rule_df[conditions]['reviewId']) )\n",
    "            simi_rule_df.loc[conditions, 'rule_check'] = 4\n",
    "            simi_rule_df.loc[conditions, 'rule_score'] = -1\n",
    "        else : \n",
    "            pass\n",
    "            \n",
    "        print(\"Rule_Code Number is : \", int(r_cd), \"\\t//\\tCF1\", cf1, \" :\",conditions.sum())\n",
    "    \n",
    "    return simi_rule_df\n",
    "\n",
    "#(Step 5-1) Calulating Complain Factor 1 similarity \n",
    "def cal_similarity(similarity_df):\n",
    "    key_cols = ['reviewId','asin','category','date', 'yearmonth', 'year', 'month']\n",
    "    simi_max_df   = similarity_df.groupby(key_cols+['cmpl_fc1','cf1_intersect','cat_cf1_ratio'])['cmpl_fc2_len'].max().reset_index()\n",
    "    simi_count_df = similarity_df.groupby(key_cols+['rule_score','cmpl_fc1','corpus_len'])['syn_match_len'].sum().reset_index()\n",
    "    simi_test     = pd.merge(simi_max_df,simi_count_df,how='left',on=key_cols+['cmpl_fc1'])\n",
    "    simi_test     = pd.merge(simi_test,priority_cf_dic[['cmpl_fc1','weight']],how='left',on=['cmpl_fc1'])\n",
    "    simi_test['cf2_similarity']    = (simi_test['syn_match_len'] / simi_test['cmpl_fc2_len'])#*simi_test['cat_cf1_ratio']\n",
    "    simi_test['corpus_similarity'] = (simi_test['syn_match_len'] / simi_test['corpus_len'] )\n",
    "    simi_test['similarity']        = simi_test[['cf1_intersect','cf2_similarity','cat_cf1_ratio','corpus_similarity','rule_score','weight']].sum(axis=1)\n",
    "    simi_test['rank'] = simi_test.groupby(['reviewId','category'])['similarity'].rank(ascending=False)\n",
    "    simi_test         = simi_test.sort_values(['reviewId','rank'],ascending=[True,True])\n",
    "    simi_test = simi_test.reindex(columns=key_cols+['corpus_len','cmpl_fc1','cmpl_fc2_len','syn_match_len','cf1_intersect',\\\n",
    "                                           'cat_cf1_ratio','cf2_similarity','corpus_similarity','rule_score','weight','similarity','rank'])\n",
    "    return simi_test\n",
    "\n",
    "\n",
    "##### Pipeline Start ######\n",
    "try:\n",
    "    ##(P1) Load input sql  \n",
    "    ##load AMZ reviews sql\n",
    "    review_sql,start_date,end_date,start_ym,end_ym    = sql_loader.review_input_load_sql()\n",
    "    ##########################################\n",
    "    # sql_cd 1 == stopword sql     \n",
    "    # sql_cd 2 == complain Factor sql     \n",
    "    # sql_cd 3 == bsr_brnad sql     \n",
    "    # sql_cd 4 == part_sql sql    \n",
    "    # sql_cd 5 == taxonomy rule sql \n",
    "    ##########################################\n",
    "    filter_sql    = sql_loader.load_sql(1)\n",
    "    factor_sql    = sql_loader.load_sql(2)\n",
    "    bsr_brand_sql = sql_loader.load_sql(3)\n",
    "    part_sql      = sql_loader.load_sql(4)\n",
    "    rules_sql     = sql_loader.load_sql(5)\n",
    "    print(f'''==================================================================================''')\n",
    "    print(f''' Taxonomy System target initialTime {end_ym} '{end_date}'(batch time) ''')\n",
    "    print(f''' Taxonomy System date yearmonth     between '{start_ym}' and '{end_ym}' ''')      \n",
    "    print(f'''==================================================================================''')\n",
    "    print(' (Setp 1-1) Load complain factor dataframe & multi word express')\n",
    "    ##(P2) Load complain factor dataframe & multi word express\n",
    "    cpl_factor    = convert_lowercase(bq.select_query(factor_sql))\n",
    "    cpl_factor['lemma'] = cpl_factor['synonym'].apply(lambda x : lemmatize(x))\n",
    "    cpl_factor['porter_stem'] = cpl_factor['synonym'].apply(lambda x : PorterStemmer().stem(x))\n",
    "    cmpl_fc_list  = list(set(cpl_factor['cmpl_fc1'].unique().tolist())|set(cpl_factor['cmpl_fc2'].unique().tolist())|set(cpl_factor['synonym'].unique().tolist())\\\n",
    "                         |set(cpl_factor['lemma'].unique().tolist()))\n",
    "    multi_express = list(filter(lambda x: len(x.split(' '))>1 , cmpl_fc_list))\n",
    "    ## Put this list for MWE Tokenizing\n",
    "    mwe = [tuple( f.split(' ')) for f in multi_express]\n",
    "    \n",
    "    ##(P3) Load Review & BSR & Parts & stopwords & taxonomy rule dataframe\n",
    "    bsr_brand_df   = convert_lowercase(bq.select_query(bsr_brand_sql))\n",
    "    top_brand      = list(set(bsr_brand_df['brand'].unique()))\n",
    "    df             = convert_lowercase(bq.select_query(review_sql))\n",
    "    stopword_df    = convert_lowercase(bq.select_query(filter_sql))\n",
    "    parts_df       = convert_lowercase(bq.select_query(part_sql))\n",
    "    parts_df['part_lemma'] = parts_df['part_word'].apply(lambda x : WordNetLemmatizer().lemmatize(x,pos='v'))\n",
    "    parts_list     = list(set(parts_df['part_word'].unique().tolist())|set(parts_df['part_lemma'].unique().tolist()))\n",
    "    parts_list.sort(reverse=True)\n",
    "    stop_words     = stopword_df['stopword'].tolist()\n",
    "    rules_df       = convert_lowercase(bq.select_query(rules_sql))\n",
    "    values = {'cmpl_fc2':'',\"cmpl_fc3\": 0, \"inch_len\": 0, \"rule1\": 0, \"rule2\": 0,'rule3':''}\n",
    "    rules_df = rules_df.fillna(value=values)\n",
    "    \n",
    "\n",
    "    ##(P4) Make input dataframe \n",
    "    neg_df = make_anal_df(df,0)\n",
    "    print('           complain factor             : ' ,cpl_factor.shape)\n",
    "    print(f'           all complain factor         : {len(cmpl_fc_list)}')\n",
    "    print(f'           multi_express lenth         : {len(mwe)}')\n",
    "    print('           stop_words                  : ' ,len(stop_words))\n",
    "    print('           bsr_brand_sql               : ' ,bsr_brand_df.shape)\n",
    "    print('           parts_df                    : ' ,parts_df.shape)\n",
    "    print('           Taxonomy rules_df           : ' ,rules_df.shape)\n",
    "    print('           Original review_data        : ' ,df.shape)\n",
    "    print(f'           rating(1~2) negative review : {neg_df.shape}')\n",
    "    print(f'           yeamonth new negative review_data : \\n' ,neg_df.groupby('yearmonth')['reviewId'].count().reset_index())\n",
    "    \n",
    "    \n",
    "    print(' (Setp 2-1) Tokenized & make corpus ')\n",
    "    ##(P5) Tokenized & make corpus \n",
    "    neg_df = neg_df.drop_duplicates('reviewId')\n",
    "    corpus_df = tokenized_corpus(neg_df)\n",
    "    corpus_df['inch_rule'] = corpus_df['inch_regex'].apply(lambda x : len(x))\n",
    "\n",
    "    print(' (Setp 3-1) Calculating Taxonomy similarity (CF1-CF2) ')\n",
    "    ##(P6) Calculating Taxonomy similarity (CF1-CF2)\n",
    "    category_cols  = ['reviewId','asin', 'date', 'yearmonth', 'year', 'month']\n",
    "    similarity_df2 = [taxonomy_similarity.remote(corpus_df[corpus_df['category']==i]) for i in corpus_df['category'].unique()]\n",
    "#     similarity_df2 = ray_multiprocessing_progress(similarity_df2)\n",
    "    similarity_df2 = pd.concat(ray.get(similarity_df2))\n",
    "#     similarity_df2 = taxonomy_similarity(corpus_df)\n",
    "    similarity_df2 = similarity_df2.drop_duplicates(['reviewId','cmpl_fc1','cmpl_fc2']).sort_values(['reviewId','cmpl_fc1'],ascending=[True,True])\n",
    "    similarity_df  = pd.merge(similarity_df2,neg_df[category_cols],how='left',on=['reviewId'])\n",
    "    similarity_df  = pd.merge(similarity_df,corpus_df[['reviewId','cmpl_fc3','inch_regex','inch_rule']],how='left',on=['reviewId'])\n",
    "    similarity_df['rule_check'] = 0\n",
    "    similarity_df['rule_score'] = 0\n",
    "\n",
    "    print(' (Setp 4-1) Reflect to taxonomy rules ')\n",
    "    ##(P7) Reflect to taxonomy rules\n",
    "    similarity_rule_df    = similarity_df[similarity_df['cmpl_fc1'].isin(rules_df.cmpl_fc1.unique().tolist())]\n",
    "    similarity_no_rule_df = similarity_df[~similarity_df['cmpl_fc1'].isin(rules_df.cmpl_fc1.unique().tolist())]\n",
    "\n",
    "    similarity_rule_check_df = similarity_rule_df.copy()\n",
    "    similarity_rule_check_df['rule_score'] = 0\n",
    "\n",
    "    cf2_list_df = similarity_rule_check_df.groupby(['reviewId','cmpl_fc1'])['cmpl_fc2'].apply(list).reset_index()\n",
    "    cf2_list_df.rename(columns={\"cmpl_fc2\":\"cmpl_fc2_list\"}, inplace=True)\n",
    "\n",
    "    similarity_rule_check_df = similarity_rule_check_df.merge(cf2_list_df[['reviewId', 'cmpl_fc2_list']], how='left', on='reviewId')\n",
    "    similarity_rule_check_df = taxonomy_rule_chk(similarity_rule_check_df, rules_df)\n",
    "    print(\"           Befoe Rule Filtering : \", similarity_rule_check_df.shape)\n",
    "    similarity_rule_check_df = similarity_rule_check_df[similarity_rule_check_df['rule_score']>0]\n",
    "    print(\"           After Rule Filtering : \", similarity_rule_check_df.shape)\n",
    "    final_similarity_rule_df = pd.concat([similarity_rule_check_df, similarity_no_rule_df])\n",
    "\n",
    "    print(\"           Merge                : \", final_similarity_rule_df.shape)\n",
    "    final_similarity_rule_df = final_similarity_rule_df.fillna(0)\n",
    "\n",
    "    print(' (Setp 5-1) complain Factor2 db upload ')\n",
    "    ## complain Factor2 db upload\n",
    "    cf2_simi_result = final_similarity_rule_df[[ 'reviewId','asin','category','date', 'yearmonth', 'year', 'month',\n",
    "                                                 'corpus_len', 'cmpl_fc1', 'cf1_intersect','cmpl_fc2_len', 'cmpl_fc2', 'cmpl_fc3', 'inch_rule', 'syn_intersect',\n",
    "                                                 'syn_match_len', 'cmpl_fc1_cd', 'cat_cmpl_fc2_len', 'cat_cf1_ratio','rule_score']]\n",
    "    \n",
    "    cf2_simi_result = cf2_simi_result.drop_duplicates()\n",
    "    cf2_simi_result = make_regidate(cf2_simi_result)\n",
    "    cf2_tbl_name = 'taxonomy.taxonomy_cf2_result'\n",
    "    reviewId_list = \"','\".join(cf2_simi_result.reviewId.unique())\n",
    "    ## avoid data duplicates upload delete reviewId and upload\n",
    "    bq.excute_query(f''' DELETE FROM {cf2_tbl_name} WHERE reviewId in({\"'\"+reviewId_list+\"'\"}) ''')\n",
    "    print(f'''           DELETE  '{cf2_tbl_name}' table target yearmonth between '{start_ym}' and '{end_ym}' target reviewId count {len(cf2_simi_result.reviewId.unique())}  ''')\n",
    "    bq.insert_append_query(cf2_tbl_name,cf2_simi_result)\n",
    "    print(f'          >> Success {cf2_tbl_name} DataBase Upload')\n",
    "    \n",
    "    print(' (Setp 6-1) Calculate Priority score by Complain Factor1  ')\n",
    "    ##(P8) Priority Complain Factor 1 \n",
    "    y_cf_dic   = {  'recovery'        : 0, \n",
    "                    'durability'      : 1,\n",
    "                    'defect'          : 2, \n",
    "                    'too hard'        : 3,  \n",
    "                    'too soft'        : 4, \n",
    "                    'missing parts'   : 5,\n",
    "                    'odor'            : 6,\n",
    "                    'sound'           : 7,\n",
    "                    'uncomfortable'   : 8,\n",
    "                    'size issue'      : 9,\n",
    "                    'shipping damage' : 10,\n",
    "                    'delivery'        : 11,\n",
    "                    'fiberglass'      : 12,\n",
    "                    'hard to set up'  : 13, \n",
    "                    'slipping'        : 14, \n",
    "                    'cover issue'     : 15, \n",
    "                    'customer service': 16, \n",
    "                    'springs felt'    : 17,\n",
    "                    'overall quality' : 18,\n",
    "                    'no support'      : 19,\n",
    "                    'customer error'  : 20,\n",
    "                    'structure design': 21,\n",
    "                    'others'          : 22, \n",
    "               }\n",
    "\n",
    "    priority_cf_dic = pd.DataFrame({'cmpl_fc1': list(y_cf_dic),\n",
    "                       'priority' : range(1,len(y_cf_dic)+1)})\n",
    "    priority_cf_dic['weight'] = round((1/priority_cf_dic['priority']),2)\n",
    "\n",
    "    print(' (Setp 7-1) ) Calculating final complain factor  ')\n",
    "    ##(P9) Calculating final complain factor \n",
    "    simi_result      = cal_similarity(final_similarity_rule_df)\n",
    "    print(' (Setp 8-1) ) Similarity normalization  through Softmax function  ')\n",
    "    ##(P10) Similarity normalization  through Softmax function\n",
    "    simi_result['exp_similarity']         = simi_result.groupby(['reviewId','yearmonth'])['similarity'].apply(np.exp)\n",
    "    simi_result['SUM_EXP_PROB']           = simi_result.groupby(['reviewId','yearmonth'])['exp_similarity'].transform('sum')\n",
    "    simi_result['softmax_similarity']     = simi_result['exp_similarity'] / simi_result['SUM_EXP_PROB']\n",
    "    simi_result                      = simi_result.sort_values(['yearmonth','reviewId','similarity'],ascending=[False,True,False])\n",
    "    simi_result['rank']              = simi_result.groupby(['reviewId','yearmonth'])['softmax_similarity'].rank(ascending=False)\n",
    "    simi_result['cumsum_similarity'] = simi_result.groupby(['reviewId','yearmonth'])['softmax_similarity'].cumsum()\n",
    "    simi_result = simi_result.drop(columns=['exp_similarity','SUM_EXP_PROB'])\n",
    "    simi_result      = simi_result.sort_values(['reviewId','rank'],ascending=[True,True])\n",
    "    simi_result = simi_result.reindex(columns=['reviewId', 'asin', 'category','date', 'yearmonth', 'year', 'month',  \n",
    "                                               'corpus_len', 'cmpl_fc1','cmpl_fc2_len', 'syn_match_len', 'cf1_intersect', 'cat_cf1_ratio',\n",
    "                                               'cf2_similarity', 'corpus_similarity', 'rule_score', 'weight',\n",
    "                                               'similarity', 'rank', 'softmax_similarity', 'cumsum_similarity'])\n",
    "    simi_result = simi_result.drop_duplicates()\n",
    "    simi_result = make_regidate(simi_result)\n",
    "    # complain Factor1 db upload\n",
    "    print(' (Setp 9-1)  complain Factor1 db upload ')\n",
    "    cf1_tbl_name = 'taxonomy.taxonomy_cf1_result'\n",
    "    reviewId_list = \"','\".join(simi_result.reviewId.unique())\n",
    "    ## avoid data duplicates upload delete reviewId and upload\n",
    "    bq.excute_query(f''' DELETE FROM {cf1_tbl_name} WHERE reviewId in({\"'\"+reviewId_list+\"'\"}) ''')\n",
    "    print(f''' Upload New reviews count : {len(reviewId_list)} ''')\n",
    "    print(f'''           DELETE  '{cf1_tbl_name}' table target yearmonth between '{start_ym}' and '{end_ym}' target reviewId count {len(simi_result.reviewId.unique())}  ''')\n",
    "    bq.insert_append_query(cf1_tbl_name,simi_result)\n",
    "    print(f'           >> Success Taxonomy Pipeline')\n",
    "    print(f'''           >>Success dataframe({simi_result.shape}) '{cf1_tbl_name}' DataBase Upload''')\n",
    "except Exception as e:\n",
    "    print(f'Error : {e}')\n",
    "finally :\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
